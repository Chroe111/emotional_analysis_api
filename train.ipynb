{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Train/Dev/Test</th>\n",
       "      <th>Writer_Joy</th>\n",
       "      <th>Writer_Sadness</th>\n",
       "      <th>Writer_Anticipation</th>\n",
       "      <th>Writer_Surprise</th>\n",
       "      <th>Writer_Anger</th>\n",
       "      <th>Writer_Fear</th>\n",
       "      <th>...</th>\n",
       "      <th>Reader3_Disgust</th>\n",
       "      <th>Reader3_Trust</th>\n",
       "      <th>Avg. Readers_Joy</th>\n",
       "      <th>Avg. Readers_Sadness</th>\n",
       "      <th>Avg. Readers_Anticipation</th>\n",
       "      <th>Avg. Readers_Surprise</th>\n",
       "      <th>Avg. Readers_Anger</th>\n",
       "      <th>Avg. Readers_Fear</th>\n",
       "      <th>Avg. Readers_Disgust</th>\n",
       "      <th>Avg. Readers_Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ぼけっとしてたらこんな時間｡チャリあるから食べにでたいのに…</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/07/31 23:48</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今日の月も白くて明るい。昨日より雲が少なくてキレイな? と立ち止まる帰り道｡チャリなし生活も...</td>\n",
       "      <td>1</td>\n",
       "      <td>2012/08/02 23:09</td>\n",
       "      <td>train</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  UserID  \\\n",
       "0                     ぼけっとしてたらこんな時間｡チャリあるから食べにでたいのに…       1   \n",
       "1  今日の月も白くて明るい。昨日より雲が少なくてキレイな? と立ち止まる帰り道｡チャリなし生活も...       1   \n",
       "\n",
       "           Datetime Train/Dev/Test  Writer_Joy  Writer_Sadness  \\\n",
       "0  2012/07/31 23:48          train           0               1   \n",
       "1  2012/08/02 23:09          train           3               0   \n",
       "\n",
       "   Writer_Anticipation  Writer_Surprise  Writer_Anger  Writer_Fear  ...  \\\n",
       "0                    2                1             1            0  ...   \n",
       "1                    3                0             0            0  ...   \n",
       "\n",
       "   Reader3_Disgust  Reader3_Trust  Avg. Readers_Joy  Avg. Readers_Sadness  \\\n",
       "0                1              0                 0                     2   \n",
       "1                0              1                 1                     0   \n",
       "\n",
       "   Avg. Readers_Anticipation  Avg. Readers_Surprise  Avg. Readers_Anger  \\\n",
       "0                          0                      0                   0   \n",
       "1                          0                      2                   0   \n",
       "\n",
       "   Avg. Readers_Fear  Avg. Readers_Disgust  Avg. Readers_Trust  \n",
       "0                  0                     0                   0  \n",
       "1                  0                     0                   0  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas.DataFrameとして読み込む\n",
    "import pandas as pd\n",
    "df_wrime = pd.read_table('wrime-ver1.tsv')\n",
    "df_wrime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259259259259259"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 教師データの割合\n",
    "train_ratio = len(df_wrime[df_wrime['Train/Dev/Test'] == 'train']) / len(df_wrime)\n",
    "train_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plutchikの8つの基本感情\n",
    "emotion_names = ['Joy', 'Sadness', 'Anticipation', 'Surprise', 'Anger', 'Fear', 'Disgust', 'Trust']\n",
    "\n",
    "# 客観感情の平均（\"Avg. Readers_*\"） の値をlist化し、新しい列として定義する\n",
    "df_wrime['readers_emotion_intensities'] = df_wrime.apply(lambda x: [x['Avg. Readers_' + name] for name in emotion_names], axis=1)\n",
    "\n",
    "# 感情強度が低いサンプルは除外する\n",
    "# (readers_emotion_intensities の max が２以上のサンプルのみを対象とする)\n",
    "is_target = df_wrime['readers_emotion_intensities'].map(lambda x: max(x) >= 2)\n",
    "df_wrime_target = df_wrime[is_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 16886\n",
      "test : 1351\n"
     ]
    }
   ],
   "source": [
    "# train / test をランダムに設定\n",
    "df_random = df_wrime_target.sample(frac=1, random_state=888)\n",
    "train_n = int(len(df_random) * train_ratio)\n",
    "df_train = df_random[:train_n]\n",
    "df_test = df_random[train_n:]\n",
    "\n",
    "print('train :', len(df_train))\n",
    "print('test :', len(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face (Transformers) 関連のモジュール\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するモデルを指定して、Tokenizerを読み込む\n",
    "checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7ee842bcaa70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddf2d46093e4d9493b872f3493db690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c4d78280fe4d4db7a6281443e77da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 前処理関数: tokenize_function\n",
    "# 感情強度の正規化（総和=1）も同時に実施する\n",
    "def tokenize_function(batch):\n",
    "    tokenized_batch = tokenizer(batch['Sentence'], truncation=True, padding='max_length')\n",
    "    tokenized_batch['labels'] = [x / np.sum(x) for x in batch['readers_emotion_intensities']]  # 総和=1に正規化\n",
    "    return tokenized_batch\n",
    "\n",
    "# Transformers用のデータセット形式に変換\n",
    "# pandas.DataFrame -> datasets.Dataset\n",
    "target_columns = ['Sentence', 'readers_emotion_intensities']\n",
    "train_dataset = Dataset.from_pandas(df_train[target_columns])\n",
    "test_dataset = Dataset.from_pandas(df_test[target_columns])\n",
    "\n",
    "# 前処理（tokenize_function） を適用\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類モデルのため AutoModelForSequenceClassification を使用する\n",
    "# checkpoint と num_labels（クラス数） を指定する. 今回は、いずれも上で定義済み\n",
    "# - checkpoint = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "num_labels = 8\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4198/3063707975.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "# 評価指標を定義\n",
    "# https://huggingface.co/docs/transformers/training\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    label_ids = np.argmax(labels, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiratori/ドキュメント/workspace/emotional_analysis_api/.venv/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2111' max='2111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2111/2111 03:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285060</td>\n",
       "      <td>0.532198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.266471</td>\n",
       "      <td>0.592154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.262927</td>\n",
       "      <td>0.638046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.250332</td>\n",
       "      <td>0.645448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.247243</td>\n",
       "      <td>0.661732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.243314</td>\n",
       "      <td>0.672835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.240951</td>\n",
       "      <td>0.688379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.235997</td>\n",
       "      <td>0.689859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.233832</td>\n",
       "      <td>0.693560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.236700</td>\n",
       "      <td>0.231279</td>\n",
       "      <td>0.701702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2111, training_loss=0.26038405862480246, metrics={'train_runtime': 228.9788, 'train_samples_per_second': 73.745, 'train_steps_per_second': 9.219, 'total_flos': 4443132626485248.0, 'train_loss': 0.26038405862480246, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練時の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\", eval_steps=200)  # 200ステップ毎にテストデータで評価する\n",
    "\n",
    "# Trainerを生成\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 訓練を実行\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Joy': 0.5719584, 'Sadness': 0.029462451, 'Anticipation': 0.30483323, 'Surprise': 0.05859948, 'Anger': 0.0025501868, 'Fear': 0.014043221, 'Disgust': 0.0115056485, 'Trust': 0.0070473277}\n"
     ]
    }
   ],
   "source": [
    "# ソフトマックス関数\n",
    "# https://www.delftstack.com/ja/howto/numpy/numpy-softmax/\n",
    "def np_softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x\n",
    "\n",
    "def analyze_emotion(text):\n",
    "    # 推論モードを有効化\n",
    "    model.eval()\n",
    "\n",
    "    # 入力データ変換 + 推論\n",
    "    tokens = tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
    "    tokens.to(model.device)\n",
    "    preds = model(**tokens)\n",
    "    prob = np_softmax(preds.logits.cpu().detach().numpy()[0])\n",
    "    out_dict = {n: p for n, p in zip(emotion_names, prob)}\n",
    "\n",
    "    print(out_dict)\n",
    "\n",
    "# 使用例 : analyze_emotion('今日から長期休暇だぁーーー！！！')\n",
    "analyze_emotion('今日から長期休暇だぁーーー！！！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
